{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nepgpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters, context_length, **kwargs):\n",
    "\n",
    "        self.characters  = characters\n",
    "        self.model_max_length = context_length\n",
    "\n",
    "        # Let's Add Custom Tokens to Tokenizer\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[EOS]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "        \n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[BOS]\": 0,\n",
    "            \"[EOS]\": 1,\n",
    "            \"[SEP]\": 2,\n",
    "            \"[CLS]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\" : 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)} \n",
    "        }\n",
    "\n",
    "        self._vocab_int_to_str = {\n",
    "            v: k for k, v in self._vocab_str_to_int.items()\n",
    "        }\n",
    "\n",
    "        super().__init__(\n",
    "        bos_token = bos_token, \n",
    "        eos_token = eos_token ,\n",
    "        sep_token = sep_token, \n",
    "        cls_token = cls_token, \n",
    "        pad_token = pad_token, \n",
    "        unk_token = unk_token,\n",
    "        mask_token = mask_token,\n",
    "        add_prefix_space=False,\n",
    "        model_max_length=context_length,\n",
    "        **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    # Get the vocabulary size\n",
    "    def vocab_size(self):\n",
    "        return len(self._vocab_str_to_int)\n",
    "    \n",
    "    # Tokenize the characters \n",
    "    def _tokenize(self, text):\n",
    "        return list(text)\n",
    "    \n",
    "    # Convert token to token_id\n",
    "    # If token doesn't exist in vocab it returns id for UNK token\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "    \n",
    "    # Convert token_id to token\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self._vocab_int_to_str[index]\n",
    "    \n",
    "    # Convert tokens to string\n",
    "    # The \"\" part means there’s no separator between tokens, so it will simply join them in the order they appear.\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "    \n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1 = None):\n",
    "\n",
    "        # token_ods_0 : Means First list of tokens\n",
    "        # token_ids_1 : Means Second list of tokens \n",
    "        \n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = cls + token_ids_0 + sep\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "    \n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1 = None, already_has_special_tokens = False):\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "     \n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1) + [1])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1 = None):\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "\n",
    "        return result\n",
    "    \n",
    "    # Get the tokenzier configuration\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(o) for o in config[\"char_ords\"]]\n",
    "        cfg[\"context_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "    \n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory, **kwargs):\n",
    "        cfg_file = Path(save_directory)/\"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Returns the vocabulary dictionary: token (str) -> id (int).\"\"\"\n",
    "        return self._vocab_str_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Initialization (`__init__`)**\n",
    "```python\n",
    "characters = ['a', 'b', 'c', ..., 'z']\n",
    "context_length = 128\n",
    "tokenizer = CharacterTokenizer(characters, context_length)\n",
    "```\n",
    "- **Input**:\n",
    "  - `characters`: List of all valid characters (e.g., `['a', 'b', ..., 'z']`).\n",
    "  - `context_length`: Maximum number of tokens a sequence can have (e.g., `128`).\n",
    "- **Explanation**:\n",
    "  - Special tokens like `[BOS]`, `[EOS]`, `[PAD]` are initialized and added to the vocabulary.\n",
    "  - `_vocab_str_to_int`: A mapping of tokens (e.g., `'a'` → 7, `'b'` → 8).\n",
    "  - `_vocab_int_to_str`: Reverse mapping (e.g., `7` → `'a'`).\n",
    "- **Example Vocabulary**:\n",
    "  ```python\n",
    "  {\n",
    "      \"[BOS]\": 0,\n",
    "      \"[EOS]\": 1,\n",
    "      \"[SEP]\": 2,\n",
    "      \"a\": 7,\n",
    "      \"b\": 8,\n",
    "      ...\n",
    "  }\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `vocab_size` Property**\n",
    "Returns the size of the vocabulary:\n",
    "```python\n",
    "tokenizer.vocab_size  # Example output: 33 (special tokens + 26 letters of alphabet)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `_tokenize()`**\n",
    "Tokenizes input text into a list of characters:\n",
    "```python\n",
    "tokenizer._tokenize(\"abc\")  # Output: ['a', 'b', 'c']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `_convert_token_to_id()`**\n",
    "Converts a token to its integer ID:\n",
    "```python\n",
    "tokenizer._convert_token_to_id(\"a\")  # Output: 7\n",
    "tokenizer._convert_token_to_id(\"[UNK]\")  # Output: 6 (default ID for unknown tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. `_convert_id_to_token()`**\n",
    "Converts an ID back to its token:\n",
    "```python\n",
    "tokenizer._convert_id_to_token(7)  # Output: \"a\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. `convert_tokens_to_string()`**\n",
    "Joins tokens into a string:\n",
    "```python\n",
    "tokens = [\"a\", \"b\", \"c\"]\n",
    "tokenizer.convert_tokens_to_string(tokens)  # Output: \"abc\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. `build_inputs_with_special_tokens()`**\n",
    "Adds special tokens `[CLS]` and `[SEP]` to a sequence of token IDs:\n",
    "```python\n",
    "token_ids_0 = [7, 8, 9]  # 'abc'\n",
    "token_ids_1 = [10, 11]   # 'de'\n",
    "tokenizer.build_inputs_with_special_tokens(token_ids_0, token_ids_1)\n",
    "# Output: [3, 7, 8, 9, 2, 10, 11, 2]  # [CLS] a b c [SEP] d e [SEP]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. `get_special_tokens_mask()`**\n",
    "Creates a mask to identify special tokens:\n",
    "```python\n",
    "token_ids_0 = [7, 8, 9]\n",
    "tokenizer.get_special_tokens_mask(token_ids_0)\n",
    "# Output: [1, 0, 0, 0, 1]  # 1 for special tokens, 0 for normal tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. `create_token_type_ids_from_sequences()`**\n",
    "Generates token type IDs for distinguishing multiple sequences:\n",
    "```python\n",
    "token_ids_0 = [7, 8, 9]  # Sequence 1\n",
    "token_ids_1 = [10, 11]   # Sequence 2\n",
    "tokenizer.create_token_type_ids_from_sequences(token_ids_0, token_ids_1)\n",
    "# Output: [0, 0, 0, 0, 0, 1, 1, 1]  # 0 for seq 1, 1 for seq 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. `get_config()`**\n",
    "Returns tokenizer configuration (characters and max length):\n",
    "```python\n",
    "tokenizer.get_config()\n",
    "# Output: {'char_ords': [97, 98, ..., 122], 'model_max_length': 128}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **11. `from_config()`**\n",
    "Reconstructs the tokenizer from a configuration:\n",
    "```python\n",
    "config = tokenizer.get_config()\n",
    "new_tokenizer = CharacterTokenizer.from_config(config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Saving and Loading**\n",
    "- **Save**: Save tokenizer configuration to a file:\n",
    "  ```python\n",
    "  tokenizer.save_pretrained(\"path/to/save\")\n",
    "  ```\n",
    "- **Load**: Load tokenizer configuration from a file:\n",
    "  ```python\n",
    "  loaded_tokenizer = CharacterTokenizer.from_pretrained(\"path/to/save\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **13. `get_vocab()`**\n",
    "Returns the vocabulary:\n",
    "```python\n",
    "tokenizer.get_vocab()\n",
    "# Output: {\"[BOS]\": 0, \"[EOS]\": 1, ..., \"a\": 7, \"b\": 8, ...}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example Usage:\n",
    "```python\n",
    "input_text = \"abc\"\n",
    "tokens = tokenizer._tokenize(input_text)  # ['a', 'b', 'c']\n",
    "token_ids = [tokenizer._convert_token_to_id(t) for t in tokens]  # [7, 8, 9]\n",
    "special_tokens = tokenizer.build_inputs_with_special_tokens(token_ids)  # [3, 7, 8, 9, 2]\n",
    "decoded = tokenizer.convert_tokens_to_string([tokenizer._convert_id_to_token(id) for id in special_tokens])\n",
    "# Output: '[CLS]abc[SEP]'\n",
    "```\n",
    "\n",
    "This implementation customizes how text is tokenized, encoded, and decoded for a model designed to process character-level input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character set and context length\n",
    "characters = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"+\", \" \", \"=\"]\n",
    "\n",
    "# Context length\n",
    "context_length = 32\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = CharacterTokenizer(characters=characters, context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[BOS]': 0,\n",
       " '[EOS]': 1,\n",
       " '[SEP]': 2,\n",
       " '[CLS]': 3,\n",
       " '[PAD]': 4,\n",
       " '[RESERVED]': 5,\n",
       " '[UNK]': 6,\n",
       " '0': 7,\n",
       " '1': 8,\n",
       " '2': 9,\n",
       " '3': 10,\n",
       " '4': 11,\n",
       " '5': 12,\n",
       " '6': 13,\n",
       " '7': 14,\n",
       " '8': 15,\n",
       " '9': 16,\n",
       " '+': 17,\n",
       " ' ': 18,\n",
       " '=': 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get vocab\n",
    "tokenizer_vocab = tokenizer.get_vocab()\n",
    "tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tokenization\n",
    "text = \"1 + 2 = 3\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1', ' ', '+', ' ', '2', ' ', '=', ' ', '3'],\n",
       " [8, 18, 17, 18, 9, 18, 19, 18, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 8, 18, 17, 18, 9, 18, 19, 18, 10, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens with special tokens\n",
    "special_tokens = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 2 = 3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokens to string\n",
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]1 + 2 = 3[SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert token IDs to tokens\n",
    "tokens_with_special_tokens = [tokenizer._convert_id_to_token(id) for id in special_tokens]\n",
    "\n",
    "# Convert tokens to string with special tokens\n",
    "tokenizer.convert_tokens_to_string(tokens_with_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 20\n",
      "Tokenizer Config: {'char_ords': [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 43, 32, 61], 'model_max_length': 32}\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab size and tikenizer configuration\n",
    "vocab_size = tokenizer.vocab_size\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Tokenizer Config: {tokenizer_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            tokenizer,\n",
    "            context_length, \n",
    "            numbers = range(100,200),\n",
    "            include_intermediate_steps = True):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.numbers = numbers\n",
    "        self.include_intermediate_steps = include_intermediate_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers) ** 2\n",
    "    \n",
    "    # Get the sample pair of numbers  \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Find the maximum number of digits from the numbers\n",
    "        max_digits = len(str(max(self.numbers)))\n",
    "        \n",
    "        # Calculate the first number by integer division of index by the length of the numbers list\n",
    "        # Calculate the second number by modulo operation of index by the length of the numbers list\n",
    "        a = self.numbers[index // len(self.numbers)]\n",
    "        b = self.numbers[index % len(self.numbers)]\n",
    "\n",
    "        # calculate the sum of the two numbers\n",
    "        c = a + b\n",
    "\n",
    "        # Pad c with leading zeros to match max_digits\n",
    "        c_str = str(c).zfill(max_digits) \n",
    "\n",
    "        # Convert c to its ones, tens, hundreds, etc. digits\n",
    "        # Say c = 123, then\n",
    "        # - Convert to string and reverse: `'321'`\n",
    "        # - Enumerate: `[(0, '3'), (1, '2'), (2, '1')]`\n",
    "        # - Convert and multiply: `[3 * 10^0, 2 * 10^1, 1 * 10^2]` which results in `[3, 20, 100]`\n",
    "        \n",
    "        c_parts = [int(d) * 10 ** i for i, d in enumerate(c_str[::-1])]\n",
    "\n",
    "        x = f\"{a}+{b}=\"  # x = \"123+456=\"\n",
    "\n",
    "        if self.include_intermediate_steps:\n",
    "            \n",
    "            # x = \"123+456=9+70+500=\" if include_intermediate_steps is True\n",
    "            x += \"+\".join([f\"{p}\" for p in c_parts]) + \"=\"\n",
    "\n",
    "        # x = \"123+456=579\" if include_intermediate_steps is False\n",
    "        x += f\"{c}\"\n",
    "\n",
    "        # the length of x should be less than or equal to the context length\n",
    "        if len(x) > self.context_length:\n",
    "            raise ValueError(f\"Input length {len(x)} exceeds context length {self.context_length}\")\n",
    "        \n",
    "        # Predict the next token in the sequence\n",
    "        y = x[1:] # y = \"23+456=9+70+500=579\"\n",
    "\n",
    "        # Tokenize the input and output sequences\n",
    "        x = self.tokenizer.encode(x)\n",
    "        y = self.tokenizer.encode(y)\n",
    "\n",
    "        # Pad the sequences to the context length\n",
    "        x = x + [self.tokenizer.pad_token_id] * (self.context_length - len(x))\n",
    "        y = y + [self.tokenizer.pad_token_id] * (self.context_length - len(y))\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Mask the output tokens with -1 to only calculate loss on the other tokens\n",
    "        mask_start = len(f\"{a}+{b}=\")\n",
    "        y[:mask_start] = -1\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Samples \n",
    "---\n",
    "### 1. **Explanation of `__len__`**\n",
    "\n",
    "The `__len__` method defines the total number of samples in your dataset. In your case:\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.numbers) ** 2\n",
    "```\n",
    "\n",
    "- `len(self.numbers)` is the size of the range provided in the `numbers` attribute.\n",
    "- For `numbers=range(100, 200)`, the size of `numbers` is $ 200 - 100 = 100 $.\n",
    "- Therefore, `len(self.numbers) ** 2` evaluates to:\n",
    "  $$\n",
    "  100^2 = 10,000 \\text{ samples.}\n",
    "  $$\n",
    "\n",
    "This means your dataset will be treated as having 10,000 samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Impact on the DataLoader**\n",
    "\n",
    "The PyTorch `DataLoader` divides the dataset into batches based on the `batch_size`. The total number of batches is:\n",
    "$$\n",
    "\\text{Number of batches} = \\frac{\\text{Total samples}}{\\text{Batch size}}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "- **Total samples (`__len__`)** = 10,000 (as defined by your dataset's `__len__` method).\n",
    "- **Batch size** = 2 (as defined in the `dataloader`).\n",
    "\n",
    "The number of batches becomes:\n",
    "$$\n",
    "\\text{Number of batches} = \\frac{10,000}{2} = 5,000\n",
    "$$\n",
    "\n",
    "This directly explains why `len(dataloader)` returns **5,000**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Why is `__len__` Squared?**\n",
    "The design choice to square the length of `numbers` (`len(self.numbers) ** 2`) likely stems from how your dataset generates samples. For example:\n",
    "- If each number in `numbers` is combined with every other number (e.g., for pairwise operations like addition), the total number of samples would be the square of the number of elements in `numbers`.\n",
    "\n",
    "For $ \\text{numbers} = [100, 101, 102, \\ldots, 199] $:\n",
    "- The dataset would generate all combinations of these numbers, resulting in:\n",
    "  $$\n",
    "  100 \\times 100 = 10,000 \\text{ samples.}\n",
    "  $$\n",
    "---\n",
    "\n",
    "## `__getitem__` function\n",
    "### 1. **How the `index` is Passed**\n",
    "1. **When Using a `DataLoader`:**\n",
    "   - When you pass the `AdditionDataset` instance to a PyTorch `DataLoader`, the `DataLoader` automatically generates indices for the dataset.\n",
    "   - These indices are passed to the `__getitem__` method by the `DataLoader` for each batch.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   from torch.utils.data import DataLoader\n",
    "\n",
    "   # Create an instance of the dataset\n",
    "   dataset = AdditionDataset(tokenizer=tokenizer, context_length=32, numbers=range(100, 200))\n",
    "\n",
    "   # Create a DataLoader\n",
    "   dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "   # Iterate over the dataloader\n",
    "   for x, y in dataloader:\n",
    "       print(x, y)  # __getitem__ is called for each batch\n",
    "   ```\n",
    "\n",
    "   - **What Happens Internally:**\n",
    "     - The `DataLoader` internally creates indices (e.g., `[0, 1, 2, ...]`). Numer of elements in list `[0, 1, 2, ...]` is equal to number of samples i.e `len(self.numbers) ** 2`.\n",
    "     - It passes these indices to the dataset's `__getitem__` method.\n",
    "     - For a batch size of 2, `__getitem__` will take indices `[0, 1]` for first batch and for second batch it will take `[2,3]` and so on. \n",
    "     - `index` argument is used by `__getitem__` to select the numbers `a` and `b` based on the logic:\n",
    "        ```python\n",
    "        a = self.numbers[index // len(self.numbers)]\n",
    "        b = self.numbers[index % len(self.numbers)]\n",
    "        ```\n",
    "---\n",
    "### 2. How `a`  and `b` are choosen and input and target are created?\n",
    "Assume the `numbers` list is `[100, 101, 102, 103, 104]` and the `index` is `7`.\n",
    "\n",
    "1. **Initialization**:\n",
    "    ```python\n",
    "    numbers = [100, 101, 102, 103, 104]\n",
    "    index = 7\n",
    "    ```\n",
    "\n",
    "2. **Calculate the first number (`a`)**:\n",
    "    ```python\n",
    "    a = numbers[index // len(numbers)]\n",
    "    ```\n",
    "    - `index // len(numbers)` calculates the integer division of `index` by the length of the `numbers` list.\n",
    "    - `len(numbers)` is `5`.\n",
    "    - `index // len(numbers)` is `7 // 5` which equals `1`.\n",
    "    - `a = numbers[1]` which is `101`.\n",
    "\n",
    "3. **Calculate the second number (`b`)**:\n",
    "    ```python\n",
    "    b = numbers[index % len(numbers)]\n",
    "    ```\n",
    "    - `index % len(numbers)` calculates the modulo of `index` by the length of the `numbers` list.\n",
    "    - `index % len(numbers)` is `7 % 5` which equals `2`.\n",
    "    - `b = numbers[2]` which is `102`.\n",
    "\n",
    "4. **Calculate the sum (c)**:\n",
    "    ```python\n",
    "    c = a + b\n",
    "    ```\n",
    "    - c = 101 + 102 which equals `203`.\n",
    "\n",
    "5. **Return the sum (c)**:\n",
    "    ```python\n",
    "    return c\n",
    "    ```\n",
    "    So, when the `__getitem__` method is called with `index = 7`, the steps are as follows:\n",
    "\n",
    "    - `a` is calculated as `101`.\n",
    "    - `b` is calculated as `102`.\n",
    "    - `c` is calculated as `203`.\n",
    "    \n",
    "6. `c_parts = [int(d) * 10 ** i for i, d in enumerate(str(c)[::-1])]` is converting a number `c`\n",
    " into its constituent digits, each multiplied by its corresponding place value (ones, tens, hundreds, etc.). Here's a step-by-step explanation:\n",
    " if `c` is `123`, the steps would be:\n",
    "    - Convert to string and reverse: `'321'`\n",
    "    - Enumerate: `[(0, '3'), (1, '2'), (2, '1')]`\n",
    "    - Convert and multiply: `[3 * 10^0, 2 * 10^1, 1 * 10^2]` which results in `[3, 20, 100]`\n",
    "    \n",
    "So, `c_parts` will be `[3, 20, 100]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Input String\n",
    "if `self.include_intermediate_steps` is `False` input will be `x = \"123+456=\"` but if `self.include_intermediate_steps` is `True` then input will be `x = \"101+102=3+00+200=\"`. The `x` will be tokenized using the tokenizer as `x = self.tokenizer.encode(x)` and pad it to context length as `x = x + [self.tokenizer.pad_token_id] * (self.context_length - len(x))`. \n",
    "\n",
    "Example: For `x = 44+354=8+90+300=398`\n",
    "```python\n",
    "x = [ 3 11 11 17 10 12 11 19 15 17 16  7 17 10  7  7 19 10 16 15  2  4  4  4  4  4  4  4  4  4  4  4]\n",
    "```\n",
    "---\n",
    "### 4. Target String\n",
    "Target string `y` is shifted by one character to predict the next token `y = \"01+102=3+0+200=203\"` (`self.include_intermediate_steps` is `True`). Then it is tokenized using the tokenizer as `y = self.tokenizer.encode(y)` and pad it to context length as `y = y + [self.tokenizer.pad_token_id] * (self.context_length - len(y))`. \n",
    "\n",
    "For the target sequence `y`, mask tokens corresponding to the input prompt (a+b=) with -1 so that the model doesn't calculate loss for those tokens. For `y = 4+354=8+90+300=398`:\n",
    "```python \n",
    "y = [-1 -1 -1 -1 -1 -1 -1 15 17 16  7 17 10  7  7 19 10 16 15  2  4  4  4  4  4  4  4  4  4  4  4  4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = AdditionDataset(tokenizer, context_length, numbers=range(100, 200), include_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "def visualize_dataset(dataset, tokenizer, max_num=10, seed=42 , skip_special_tokens=True):\n",
    "    \n",
    "    # Generate random indices\n",
    "    indices = torch.randperm(\n",
    "        len(dataset), generator=torch.Generator().manual_seed(seed)\n",
    "    )[:max_num]\n",
    "\n",
    "    # Retrieve input-target pairs for the selected indices\n",
    "    for ix in indices:\n",
    "        x, y = dataset[ix]\n",
    "\n",
    "        print(f\"=== Example {ix.item()} ===\")  # Ensure index is displayed as a number\n",
    "\n",
    "        # Configure NumPy to display arrays without truncation\n",
    "        np.set_printoptions(linewidth=999)\n",
    "\n",
    "        # Print the input tensor\n",
    "        print(f\"x = {x.numpy()}\")\n",
    "        print(f\"y = {y.numpy()}\")\n",
    "\n",
    "        # Show lengths of x and y\n",
    "        print(f\"x length = {len(x)}\")\n",
    "        print(f\"y length = {len(y)}\")\n",
    "\n",
    "        # Decode x and print\n",
    "        x_decoded = tokenizer.decode(x.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "        print(f\"x decoded = {x_decoded}\")\n",
    "\n",
    "        # Decode y and print, replacing -1 tokens with '_'\n",
    "        y_list = y.tolist()\n",
    "        num_unknowns = y_list.count(-1)\n",
    "        if num_unknowns > 0:\n",
    "            y_decoded = tokenizer.decode(y_list[num_unknowns:], skip_special_tokens=skip_special_tokens)\n",
    "            print(f\"y decoded = {'_' * num_unknowns}{y_decoded}\")\n",
    "        else:\n",
    "            y_decoded = tokenizer.decode(y_list, skip_special_tokens=skip_special_tokens)\n",
    "            print(f\"y decoded = {y_decoded}\")\n",
    "\n",
    "        print(\"\\n\")  # Add a blank line between examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset\n",
    "\n",
    "#### **1. Generate Random Indices**\n",
    "```python\n",
    "indices = torch.randperm(\n",
    "    len(dataset), generator=torch.Generator().manual_seed(seed)\n",
    ")[:max_num]\n",
    "```\n",
    "- `torch.randperm(len(dataset))`: Creates a random permutation of indices based on the dataset size.\n",
    "- `manual_seed(seed)`: Ensures reproducibility.\n",
    "- `[:max_num]`: Selects the first `max_num` random indices.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Loop Through the Selected Indices**\n",
    "```python\n",
    "for ix in indices:\n",
    "    x, y = dataset[ix]\n",
    "```\n",
    "For each selected index:\n",
    "- Retrieve the input (`x`) and target (`y`) tensors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Print Tensor Values**\n",
    "```python\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = {y.numpy()}\")\n",
    "```\n",
    "- Converts the tensors into NumPy arrays and prints their raw numerical values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Show Lengths of Tensors**\n",
    "```python\n",
    "print(f\"x length = {len(x)}\")\n",
    "print(f\"y length = {len(y)}\")\n",
    "```\n",
    "- Displays the length of each sequence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Decode `x`**\n",
    "```python\n",
    "x_decoded = tokenizer.decode(x.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "print(f\"x decoded = {x_decoded}\")\n",
    "```\n",
    "- Converts the numerical `x` sequence back to a human-readable string using the tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Decode `y`**\n",
    "```python\n",
    "y_list = y.tolist()\n",
    "num_unknowns = y_list.count(-1)\n",
    "if num_unknowns > 0:\n",
    "    y_decoded = tokenizer.decode(y_list[num_unknowns:], skip_special_tokens=skip_special_tokens)\n",
    "    print(f\"y decoded = {'_' * num_unknowns}{y_decoded}\")\n",
    "else:\n",
    "    y_decoded = tokenizer.decode(y_list, skip_special_tokens=skip_special_tokens)\n",
    "    print(f\"y decoded = {y_decoded}\")\n",
    "```\n",
    "- `y_list.count(-1)`: Counts how many tokens in `y` are masked (`-1`).\n",
    "- Decodes only the unmasked portion of `y`.\n",
    "- Replaces the masked portion with `_`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 7542 ===\n",
      "x = [ 3  8 14 12 17  8 11  9 19 14 17  8  7 17 10  7  7 19 10  8 14  2  4  4  4  4  4  4  4  4  4  4]\n",
      "y = [-1 -1 -1 -1 -1 -1 -1 -1 14 17  8  7 17 10  7  7 19 10  8 14  2  4  4  4  4  4  4  4  4  4  4  4]\n",
      "x length = 32\n",
      "y length = 32\n",
      "x decoded = 175+142=7+10+300=317\n",
      "y decoded = ________7+10+300=317\n",
      "\n",
      "\n",
      "=== Example 8214 ===\n",
      "x = [ 3  8 15  9 17  8  8 11 19 13 17 16  7 17  9  7  7 19  9 16 13  2  4  4  4  4  4  4  4  4  4  4]\n",
      "y = [-1 -1 -1 -1 -1 -1 -1 -1 13 17 16  7 17  9  7  7 19  9 16 13  2  4  4  4  4  4  4  4  4  4  4  4]\n",
      "x length = 32\n",
      "y length = 32\n",
      "x decoded = 182+114=6+90+200=296\n",
      "y decoded = ________6+90+200=296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_dataset(dataset, tokenizer, max_num=2, seed=42, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            drop_last=True,  \n",
    "            num_workers=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_loader(data_loader, tokenizer, max_batches=2, skip_special_tokens=True):\n",
    "    print(\"=== Data Loader ===\")\n",
    "    print(f\"Number of batches = {len(data_loader)}\\n\")\n",
    "    print(f\"Showing first {max_batches} batches:\")\n",
    "    print(f\"Number of samples in each batch = {data_loader.batch_size}\\n\")\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"=== Batch {i + 1} ===\")\n",
    "        \n",
    "        for j in range(len(x_batch)):\n",
    "            x = x_batch[j]\n",
    "            y = y_batch[j]\n",
    "            \n",
    "            print(f\"--- Sample {j + 1} ---\")\n",
    "            print(f\"x = {x.numpy()}\")\n",
    "            print(f\"y = {y.numpy()}\")\n",
    "            \n",
    "            x_decoded = tokenizer.decode(x.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "            print(f\"x decoded = {x_decoded}\")\n",
    "            \n",
    "            y_list = y.tolist()\n",
    "            num_unknowns = y_list.count(-1)\n",
    "            y_decoded = tokenizer.decode(y_list[num_unknowns:], skip_special_tokens=skip_special_tokens)\n",
    "            print(f\"y decoded = {'_' * num_unknowns}{y_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Loader ===\n",
      "Number of batches = 10000\n",
      "\n",
      "Showing first 2 batches:\n",
      "Number of samples in each batch = 1\n",
      "\n",
      "=== Batch 1 ===\n",
      "--- Sample 1 ---\n",
      "x = [ 3  8 10 15 17  8 16 10 19  8 17 10  7 17 10  7  7 19 10 10  8  2  4  4  4  4  4  4  4  4  4  4]\n",
      "y = [-1 -1 -1 -1 -1 -1 -1 -1  8 17 10  7 17 10  7  7 19 10 10  8  2  4  4  4  4  4  4  4  4  4  4  4]\n",
      "x decoded = 138+193=1+30+300=331\n",
      "y decoded = ________1+30+300=331\n",
      "\n",
      "=== Batch 2 ===\n",
      "--- Sample 1 ---\n",
      "x = [ 3  8 15 11 17  8  9  9 19 13 17  7 17 10  7  7 19 10  7 13  2  4  4  4  4  4  4  4  4  4  4  4]\n",
      "y = [-1 -1 -1 -1 -1 -1 -1 -1 13 17  7 17 10  7  7 19 10  7 13  2  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "x decoded = 184+122=6+0+300=306\n",
      "y decoded = ________6+0+300=306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_data_loader(data_loader, tokenizer, max_batches=2, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step Creation of GPT like Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Linear layers for queries, keys, and values with bias\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Linear layer for output projection\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = self.dropout(torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1))\n",
    "\n",
    "        # Context vector\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.dropout(self.out_proj(context_vec))  # Apply dropout after projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a detailed explanation of the `MultiHeadAttention` class line by line, with examples where necessary:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Class Initialization**\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "```\n",
    "- **Explanation:**\n",
    "  - `MultiHeadAttention` is a custom PyTorch module for multi-head self-attention.\n",
    "  - **Parameters:**\n",
    "    - `d_in`: Input dimension (size of each input vector).\n",
    "    - `d_out`: Output dimension (size of each output vector).\n",
    "    - `context_length`: Maximum sequence length (e.g., for input tokens).\n",
    "    - `dropout`: Dropout probability to regularize attention.\n",
    "    - `num_heads`: Number of attention heads.\n",
    "    - `qkv_bias`: Whether to include biases in the query, key, and value projections.\n",
    "  - The `assert` ensures that `d_out` is divisible by `num_heads`, as each head must process an equal portion of `d_out`.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  attn = MultiHeadAttention(d_in=64, d_out=128, context_length=32, dropout=0.1, num_heads=4)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Defining Key Components**\n",
    "```python\n",
    "self.d_out = d_out\n",
    "self.num_heads = num_heads\n",
    "self.head_dim = d_out // num_heads\n",
    "```\n",
    "- **Explanation:**\n",
    "  - `d_out`: Total output dimension.\n",
    "  - `num_heads`: Number of attention heads.\n",
    "  - `head_dim`: Dimension of each head. It is calculated as `d_out // num_heads`.\n",
    "\n",
    "- **Example:**\n",
    "  - For `d_out=128` and `num_heads=4`, `head_dim = 128 // 4 = 32`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Query, Key, Value, and Output Projection Layers**\n",
    "```python\n",
    "self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "self.out_proj = nn.Linear(d_out, d_out, bias=True)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - `W_query`, `W_key`, `W_value`: Linear layers to project the input `x` into query, key, and value spaces.\n",
    "  - `out_proj`: Linear layer to project the concatenated outputs from all heads back to the original `d_out` dimension.\n",
    "  - `dropout`: Regularizes attention weights during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Causal Mask for Attention**\n",
    "```python\n",
    "self.register_buffer(\n",
    "    \"mask\", \n",
    "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    ")\n",
    "```\n",
    "- **Explanation:**\n",
    "  - A **causal mask** ensures that the model cannot attend to future tokens.\n",
    "  - `torch.triu`: Creates an upper triangular matrix of ones (0 below the diagonal).\n",
    "  - The result is stored as a non-trainable buffer (`mask`).\n",
    "- **Example:**\n",
    "  For `context_length=4`, the mask looks like:\n",
    "  ```\n",
    "  [[0, 1, 1, 1],\n",
    "   [0, 0, 1, 1],\n",
    "   [0, 0, 0, 1],\n",
    "   [0, 0, 0, 0]]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Forward Method**\n",
    "#### **Input Shape**\n",
    "- `x`: Tensor of shape `(batch_size, num_tokens, d_in)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.1 Compute Keys, Queries, Values**\n",
    "```python\n",
    "keys = self.W_key(x)\n",
    "queries = self.W_query(x)\n",
    "values = self.W_value(x)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Projects the input `x` into keys, queries, and values using the learned linear layers.\n",
    "  - Shape after projection: `(batch_size, num_tokens, d_out)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2 Split into Multiple Heads**\n",
    "```python\n",
    "keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Reshapes the `keys`, `queries`, and `values` to include the number of heads.\n",
    "  - Shape changes:\n",
    "    - `(batch_size, num_tokens, d_out)` → `(batch_size, num_tokens, num_heads, head_dim)`.\n",
    "    - Then transposes to `(batch_size, num_heads, num_tokens, head_dim)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.3 Scaled Dot-Product Attention**\n",
    "```python\n",
    "attn_scores = queries @ keys.transpose(-2, -1)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Computes attention scores by taking the dot product between `queries` and transposed `keys`.\n",
    "  - Shape:\n",
    "    - `queries`: `(batch_size, num_heads, num_tokens, head_dim)`.\n",
    "    - `keys.transpose(-2, -1)`: `(batch_size, num_heads, head_dim, num_tokens)`.\n",
    "    - Result: `(batch_size, num_heads, num_tokens, num_tokens)`.\n",
    "\n",
    "```python\n",
    "mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Applies the causal mask to prevent attending to future tokens by replacing masked positions with `-inf`.\n",
    "\n",
    "```python\n",
    "attn_weights = self.dropout(torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1))\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Normalizes attention scores with `softmax` and scales by the square root of `head_dim` to stabilize gradients.\n",
    "  - Applies dropout to the attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.4 Compute Context Vector**\n",
    "```python\n",
    "context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Multiplies the attention weights with the `values` to compute the context vectors.\n",
    "  - Shape:\n",
    "    - `attn_weights`: `(batch_size, num_heads, num_tokens, num_tokens)`.\n",
    "    - `values`: `(batch_size, num_heads, num_tokens, head_dim)`.\n",
    "    - Result after multiplication and reshaping: `(batch_size, num_tokens, d_out)`.\n",
    "\n",
    "```python\n",
    "context_vec = self.dropout(self.out_proj(context_vec))\n",
    "```\n",
    "- **Explanation:**\n",
    "  - Applies the output projection layer (`out_proj`) and dropout.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Return the Context Vectors**\n",
    "```python\n",
    "return context_vec\n",
    "```\n",
    "- The final output is a tensor of shape `(batch_size, num_tokens, d_out)`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 16, 32])\n",
      "Output shape: torch.Size([2, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example of MultiHeadAttention\n",
    "d_in = 32 # Input Embedding Size\n",
    "d_out = 64 # Output Embedding Size\n",
    "num_heads = 4 # Number of heads\n",
    "dropout = 0.1 # Dropout rate\n",
    "batch_size = 2 # Batch size\n",
    "context_length = 16 # Number of tokens in the sequence\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads)\n",
    "\n",
    "# Create a random input tensor\n",
    "x = torch.randn(batch_size, context_length, d_in)\n",
    "attn = mha(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is meaning of calculating attention weights? \n",
    "Attention weights represent how strongly each token is related to other tokens in the sequence. They indicate how much focus or importance each token places on every other token, effectively capturing semantic relationships.\n",
    "In other words, attention weights answer the question: \"__Which tokens are important for understanding this token?__\"\n",
    "### What is the meaning of calculating context vectors?\n",
    "The context vector captures why a token is related to other tokens and in what way it is related. It provides a contextualized representation of a token, combining its own meaning with relevant information from other tokens.\n",
    "The context vector answers the question: \"__On what basis are these tokens semantically related?__\" or \"__What features or attributes describe this relationship?__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Normalization \n",
    "The idea behind the layer normalization is to adjust the activation(output) of a neural network layer to have a __mean__ of 0 and __variance__ of 1. \n",
    "\n",
    "The formula for **Layer Normalization** is as follows:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $: Input vector to be normalized (of shape $(n, d)$ where $n$ is the batch size and $d$ is the feature dimension).\n",
    "- $\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i $: Mean of the features for each input, calculated across the feature dimension.\n",
    "- $ \\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2 $: Variance of the features for each input.\n",
    "- $\\epsilon$: Small constant added to variance to avoid division by zero, typically a very small value like $^{-5}$.\n",
    "- $\\gamma$: Learnable scaling parameter (of shape $(d,)$).\n",
    "- $\\beta$: Learnable shifting parameter (of shape $(d,)$).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Normalization**: The input $x$ is normalized by subtracting the mean ($\\mu$) and dividing by the standard deviation ($\\sqrt{\\sigma^2 + \\epsilon}$). This process transforms the input to have zero mean and unit variance.\n",
    "- **Scaling and Shifting**: After normalization, the input is scaled by a learnable parameter $\\gamma$ and shifted by a learnable parameter $\\beta$. These parameters allow the model to learn an appropriate scale and shift for the normalized values, enabling the network to maintain the representational power of the original input.\n",
    "\n",
    "Layer normalization is different from batch normalization in that it normalizes across the feature dimension for each input individually rather than across the batch dimension, making it especially suitable for recurrent or sequence-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feed Forward Network\n",
    "### Gaussian Error Linear Unit(GELU) Activation Function\n",
    "The GELU activation function can be implemented in several ways. The extact version is defined as \n",
    "$$\n",
    "GELU(x) = x.\\Phi(x)\n",
    "$$\n",
    "where, $\\Phi(x)$ is the cumulative distribution fuction of the standard Gaussian distribution. \n",
    "\n",
    "The computationally cheaper approximation if above equation is \n",
    "$$\n",
    "GELU(x) \\approx 0.5\\,.\\,x\\,.\\left(1+tanh\\left[\\sqrt\\frac{2}{\\pi}.(x + 0.044715.x^3)\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "    torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "    ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # The first linear layer increases the embedding dimension by a factor of 4\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]) # The second linear layer decreases the embedding dimension by a factor of 4\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion and Contrcation of Layers\n",
    "`FeedForward` module plays a crucial role in enhancing the model's ability to learn from and generalize the data. Although the input and output dimensions of the module are the same, it internally expands the embedding dimension into a higher-dimensional space through the first linear layer as illustrated in the figure abvoe. The expansion is followed by a nonlinear GELU activation and then contraction back to the original dimesnion with the second linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Block with residual connection\n",
    "Now, we'll connect layer normalization, __GELU activations__, feed forward module and __shortcut connections__ in a transofrmer block, which is the final building block of the GPT architecture. Figure below shows a transformer block that combines several components, including the masked multi-head attention and `FeedForward` module. __Layer normalization__ is applied before each of these two components and __dropout__ is applied after them to regularize the model and prevent overfitting. This is also known as _Pre-LayerNorm_. Older architectures, such as original transformer model, applied layer normalization after the self-attention and feed forward networks instead, known as _Post-LayerNorm_, which often leads to worse traning dynamics. \n",
    "\n",
    "The transformer block maintains the input dimensions in its output, indicating that the transfomer architecture processes sequences of data without altering their shape throughout the network. Actually, the actual output is the context vector that encapsulates information from the entire input sequence. \n",
    "\n",
    "__The preservation of shape through the transformer block architecture is not incidental but a crucial aspect of its design.__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x # Shortcut connection for attention block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add original input back\n",
    "\n",
    "        shortcut = x # Shortcut connection for feed forward block\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_leng = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_leng, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        # Add token and positional embeddings, apply dropout\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # Final layer normalization and output projection\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the `TransformerBlock` class, the `GPTModel` class is realtively small and compact. \n",
    "\n",
    "The `__init__` constrcutor of this `GPTModel` class initializes the token and positional embedding layers using the configurations passed in via a Python dictionary `cfg`. These embedding layers are responsible for converting input token indices into dense vectors and adding positinal information. Next, the `__init__` method creates a sequential stack of `TransformerBlock` modules equal to the number of layers specified in `cfg`. \n",
    "\n",
    "Following the transformer blocks, a `LayerNorm` layers is applied, standardizing the outputs from the transformer blocks to stabilize the learning process. Finally, a linear output head wthout bias is defined which projects the transfomer's output inot a vocabulary space of the tokenizer to generate logits for each token in the vocabulary. \n",
    "\n",
    "The forward method takes a batch of input token indices, computes their embeddings, applies the positional embeddings, passes the sequence through the transformer blocks, normalizes the final output, and then computes the logits, representing the next token's unnormazlied probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration\n",
    "model_config = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,        # Vocabulary Size\n",
    "    \"context_length\": 32,                      # Context Length\n",
    "    \"emb_dim\" : 48,                            # Embedding dimension\n",
    "    \"n_heads\": 4,                              # Number of attention heads\n",
    "    \"n_layers\": 3,                             # Number of layers\n",
    "    \"drop_rate\": 0.1,                          # Dropout rate     \n",
    "    \"qkv_bias\":True                            # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Instantiate the GPTModel\n",
    "model = GPTModel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 88,368\n",
      "Actual Number of trainable parameters considering weight tying: 87,408\n",
      "Total size of the model is: 0.34 MB\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Number of Parameters: {total_params:,}\")\n",
    "    actual_total_params = (\n",
    "        total_params - sum(\n",
    "            p.numel() for p in model.out_head.parameters()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_size_bytes = total_params * 4\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"Actual Number of trainable parameters \"\n",
    "          f\"considering weight tying: {actual_total_params:,}\\n\"\n",
    "          f\"Total size of the model is: {total_size_mb:.2f} MB\")\n",
    "# Call the function with the model\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    test_dataset=None,\n",
    "    config=None,\n",
    "    device=None,\n",
    "    save_path=\"model.pth\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate the given model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        test_loader: DataLoader for the test dataset (optional).\n",
    "        config: Configuration dictionary containing hyperparameters like\n",
    "                learning rate, weight decay, batch size, max_epochs, etc.\n",
    "        device: Device to use ('cuda', 'mps', or 'cpu'). If None, it will be auto-detected.\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = (\n",
    "            \"cuda\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # Optimizer setup\n",
    "    def create_optimizer(model, config):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        params_decay = [\n",
    "            p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n",
    "        ]\n",
    "        params_no_decay = [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ]\n",
    "        optim_groups = [\n",
    "            {\"params\": params_decay, \"weight_decay\": config[\"weight_decay\"]},\n",
    "            {\"params\": params_no_decay, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return AdamW(optim_groups, lr=config[\"learning_rate\"], betas=config[\"betas\"])\n",
    "\n",
    "    optimizer = create_optimizer(model, config)\n",
    "\n",
    "    # Data Loader\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=config[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "   \n",
    "    test_loader = (\n",
    "        DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            drop_last=False,  \n",
    "            num_workers=config[\"num_workers\"],\n",
    "        )\n",
    "        if test_dataset\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Single epoch logic\n",
    "    def run_epoch(loader, is_train):\n",
    "        model.train(is_train)\n",
    "        total_loss = 0.0\n",
    "        loader = tqdm(loader, desc=\"Training\" if is_train else \"Evaluating\")\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                logits= model(x)\n",
    "                # loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten(), ignore_index=-1)\n",
    "                loss = torch.nn.functional.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1\n",
    "                )\n",
    "                loss = loss.mean()  # Handle multiple GPUs\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_norm_clip\"])\n",
    "                    optimizer.step()\n",
    "            loader.set_postfix(loss=loss.item())\n",
    "        return total_loss / len(loader)\n",
    "    \n",
    "    # Track training and validation loss\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"max_epochs\"]):\n",
    "        print(f\"Epoch {epoch + 1}/{config['max_epochs']}\")\n",
    "        train_loss = run_epoch(train_loader, is_train=True)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loss = run_epoch(test_loader, is_train=False)\n",
    "            valid_losses.append(test_loss)\n",
    "            print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function Header and Docstring**\n",
    "```python\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    test_dataset=None,\n",
    "    config=None,\n",
    "    device=None,\n",
    "    save_path=\"model.pth\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate the given model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        test_loader: DataLoader for the test dataset (optional).\n",
    "        config: Configuration dictionary containing hyperparameters like\n",
    "                learning rate, weight decay, batch size, max_epochs, etc.\n",
    "        device: Device to use ('cuda', 'mps', or 'cpu'). If None, it will be auto-detected.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "- **Purpose**: Defines a function to train and evaluate a model using PyTorch.\n",
    "- **Arguments**:\n",
    "  - `model`: The neural network model to train and evaluate.\n",
    "  - `train_dataset`: The training dataset.\n",
    "  - `test_dataset` (optional): The test/validation dataset.\n",
    "  - `config`: A dictionary containing training hyperparameters.\n",
    "  - `device`: The computational device (GPU/CPU) to use.\n",
    "  - `save_path`: Path to save the trained model's weights.\n",
    "\n",
    "---\n",
    "\n",
    "### **Device Selection**\n",
    "```python\n",
    "if device is None:\n",
    "    device = (\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "```\n",
    "\n",
    "- **Checks if a specific `device` is provided**. If not:\n",
    "  - Prioritizes `cuda` (NVIDIA GPU) if available.\n",
    "  - Falls back to `mps` (Apple silicon) or `cpu`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Move Model to Device**\n",
    "```python\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "```\n",
    "\n",
    "- Wraps the model in `DataParallel` to allow multi-GPU training.\n",
    "- Moves the model to the selected `device`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizer Setup**\n",
    "```python\n",
    "def create_optimizer(model, config):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    params_decay = [\n",
    "        p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n",
    "    ]\n",
    "    params_no_decay = [\n",
    "        p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "    ]\n",
    "    optim_groups = [\n",
    "        {\"params\": params_decay, \"weight_decay\": config[\"weight_decay\"]},\n",
    "        {\"params\": params_no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return AdamW(optim_groups, lr=config[\"learning_rate\"], betas=config[\"betas\"])\n",
    "```\n",
    "\n",
    "- **Purpose**: Defines an optimizer with different weight decay for parameters.\n",
    "  - `no_decay`: Specifies parameters like `bias` and `LayerNorm.weight` that should not have weight decay.\n",
    "  - Groups parameters based on their need for weight decay.\n",
    "  - Returns an `AdamW` optimizer with the specified learning rate and betas.\n",
    "\n",
    "```python\n",
    "optimizer = create_optimizer(model, config)\n",
    "```\n",
    "- Instantiates the optimizer using the `create_optimizer` function.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Loaders**\n",
    "```python\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "```\n",
    "\n",
    "- Creates a PyTorch `DataLoader` for the training dataset with:\n",
    "  - Shuffling for randomness.\n",
    "  - Dropping the last batch if it's incomplete.\n",
    "  - `num_workers`: Number of parallel data loading workers.\n",
    "\n",
    "```python\n",
    "test_loader = (\n",
    "    DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=False,  \n",
    "        num_workers=config[\"num_workers\"],\n",
    "    )\n",
    "    if test_dataset\n",
    "    else None\n",
    ")\n",
    "```\n",
    "\n",
    "- Creates a `DataLoader` for the test dataset (if provided), without shuffling and without dropping incomplete batches.\n",
    "\n",
    "---\n",
    "\n",
    "### **Single Epoch Logic**\n",
    "```python\n",
    "def run_epoch(loader, is_train):\n",
    "    model.train(is_train)\n",
    "    total_loss = 0.0\n",
    "    loader = tqdm(loader, desc=\"Training\" if is_train else \"Evaluating\")\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits= model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1\n",
    "            )\n",
    "            loss = loss.mean()  # Handle multiple GPUs\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_norm_clip\"])\n",
    "                optimizer.step()\n",
    "        loader.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(loader)\n",
    "```\n",
    "\n",
    "- **Purpose**: Executes one epoch (training or evaluation).\n",
    "  - **`is_train`**:\n",
    "    - `True`: Training mode (enables gradient computation).\n",
    "    - `False`: Evaluation mode (disables gradient computation).\n",
    "  - Uses `tqdm` for a progress bar.\n",
    "  - Iterates through batches:\n",
    "    - Moves input (`x`) and target (`y`) to the device.\n",
    "    - Computes model predictions (`logits`).\n",
    "    - Calculates cross-entropy loss (ignoring index `-1`).\n",
    "    - For training:\n",
    "      - Zeroes gradients.\n",
    "      - Backpropagates loss.\n",
    "      - Clips gradients (to avoid explosion).\n",
    "      - Updates model parameters using the optimizer.\n",
    "  - Tracks total loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Loop**\n",
    "```python\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(config[\"max_epochs\"]):\n",
    "    print(f\"Epoch {epoch + 1}/{config['max_epochs']}\")\n",
    "    train_loss = run_epoch(train_loader, is_train=True)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loss = run_epoch(test_loader, is_train=False)\n",
    "        valid_losses.append(test_loss)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "```\n",
    "\n",
    "- Tracks training and validation losses over epochs.\n",
    "- For each epoch:\n",
    "  - Runs training (`run_epoch` with `is_train=True`).\n",
    "  - Optionally runs evaluation (`run_epoch` with `is_train=False`).\n",
    "  - Logs losses.\n",
    "\n",
    "---\n",
    "\n",
    "### **Save Trained Model**\n",
    "```python\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "```\n",
    "\n",
    "- Saves the model's state dictionary (weights and biases) to the specified path.\n",
    "\n",
    "---\n",
    "\n",
    "### **Return Results**\n",
    "```python\n",
    "return train_losses, valid_losses\n",
    "```\n",
    "\n",
    "- Returns lists of training and validation losses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning Configuration\n",
    "config = {\n",
    "    \"dataset_range\": range(0,1000),  # Testing dataset range\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"batch_size\": 100,\n",
    "    \"num_workers\": 0,\n",
    "    \"grad_norm_clip\": 1.0,\n",
    "    \"max_epochs\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataloaders\n",
    "addition_dataset = AdditionDataset(\n",
    "    context_length=model_config[\"context_length\"],\n",
    "    tokenizer=tokenizer,\n",
    "    numbers=config[\"dataset_range\"],\n",
    "    include_intermediate_steps=True\n",
    ")\n",
    "\n",
    "# Define the proportions for traning and validation datasets\n",
    "train_size = int(0.8 * len(addition_dataset)) # 80% for training\n",
    "val_size = len(addition_dataset) - train_size # 20 % for validation\n",
    "\n",
    "# Split the dataset into traning and validation datasets\n",
    "train_dataset, val_dataset = random_split(\n",
    "    addition_dataset, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Samples in Train and Test Dataset\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate the Model\n",
    "train_losses, valid_losses =  train_and_evaluate(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=val_dataset,\n",
    "    config=config,\n",
    "    save_path=\"trained_model_1000_v2.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for the GPT model to generate tokens\n",
    "def generate(model, indx, max_new_tokens, temperature=1.0, top_k=None, stop_tokens=None):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        context_length = model.pos_emb.weight.shape[0]\n",
    "        # If given sequence is longer than the context length we must trim it to context length\n",
    "        if indx.shape[-1] > context_length:\n",
    "            indx = indx[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(indx)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Sample tokens to the top-k most likely tokens and exclude all other tokens\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            # Minimum value of top k logits\n",
    "            min_value = top_logits[:, -1]\n",
    "            # Set all logits less than min_value to -inf\n",
    "            logits = torch.where(logits < min_value, torch.tensor(-np.inf).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            # Apply temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            # If temperature is 0, use argmax to select the next token\n",
    "            index_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the generated token to the input sequence\n",
    "        indx = torch.cat((indx, index_next), dim=1)\n",
    "\n",
    "        # Check if the generated token is in the stop tokens\n",
    "        if stop_tokens is not None and index_next.item() in stop_tokens:\n",
    "            #print(\"Stop token generated. Ending sequence.\", index_next.item())\n",
    "            break\n",
    "\n",
    "    return indx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "This function `generate` is designed for generating a sequence of tokens using a GPT model. Here's a line-by-line breakdown of the code:\n",
    "\n",
    "---\n",
    "\n",
    "### **Function Header**\n",
    "```python\n",
    "def generate(model, indx, max_new_tokens, temperature=1.0, top_k=None, stop_tokens=None):\n",
    "```\n",
    "\n",
    "- **Purpose**: Generates a sequence of tokens using a pretrained GPT model.\n",
    "- **Parameters**:\n",
    "  - `model`: The GPT model to use for token generation.\n",
    "  - `indx`: A tensor containing the input token sequence (starting context).\n",
    "  - `max_new_tokens`: The maximum number of tokens to generate.\n",
    "  - `temperature`: Controls randomness in sampling; lower values make the model more deterministic.\n",
    "  - `top_k`: Restricts token selection to the top-k most probable tokens (for diversity control).\n",
    "  - `stop_tokens`: A list of tokens that, if generated, will stop the generation process early.\n",
    "\n",
    "---\n",
    "\n",
    "### **Token Generation Loop**\n",
    "```python\n",
    "for _ in range(max_new_tokens):\n",
    "```\n",
    "\n",
    "- Loops for up to `max_new_tokens` iterations to generate tokens one at a time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Context Trimming**\n",
    "```python\n",
    "context_length = model.pos_emb.weight.shape[0]\n",
    "if indx.shape[-1] > context_length:\n",
    "    indx = indx[:, -context_length:]\n",
    "```\n",
    "\n",
    "- **Why?** GPT models have a fixed context length (the number of tokens they can attend to at once).\n",
    "- If the input sequence (`indx`) exceeds the model's context length, it trims the input to the most recent tokens (last `context_length` tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### **Get Model Predictions**\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logits = model(indx)\n",
    "```\n",
    "\n",
    "- Passes the input sequence (`indx`) through the model to get the logits (raw predictions for each token in the vocabulary).\n",
    "- Uses `torch.no_grad()` to prevent gradient computation during inference (saves memory and computation).\n",
    "\n",
    "---\n",
    "\n",
    "### **Extract Predictions for Next Token**\n",
    "```python\n",
    "logits = logits[:, -1, :] / temperature\n",
    "```\n",
    "\n",
    "- Extracts logits for the last token in the sequence (next token prediction).\n",
    "- Divides by `temperature`:\n",
    "  - **High Temperature (>1.0)**: Makes predictions more random.\n",
    "  - **Low Temperature (<1.0)**: Focuses on high-probability predictions.\n",
    "  - **Temperature = 0**: Fully deterministic (argmax is used later).\n",
    "\n",
    "---\n",
    "\n",
    "### **Apply Top-k Sampling**\n",
    "```python\n",
    "if top_k is not None:\n",
    "    top_logits, _ = torch.topk(logits, top_k)\n",
    "    min_value = top_logits[:, -1]\n",
    "    logits = torch.where(logits < min_value, torch.tensor(-np.inf).to(logits.device), logits)\n",
    "```\n",
    "\n",
    "- **Top-k Sampling**: Limits the token selection to the top-k most probable tokens.\n",
    "  - Finds the smallest value in the top-k logits (`min_value`).\n",
    "  - Sets all other logits to `-inf`, ensuring they are ignored during sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sample Next Token**\n",
    "```python\n",
    "if temperature > 0.0:\n",
    "    logits = logits / temperature\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    index_next = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "- If `temperature > 0`, applies softmax to the logits to convert them into probabilities.\n",
    "- Uses `torch.multinomial` to randomly sample a token based on these probabilities.\n",
    "\n",
    "```python\n",
    "else:\n",
    "    index_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "```\n",
    "\n",
    "- If `temperature == 0`, selects the token with the highest probability (`argmax`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Update Input Sequence**\n",
    "```python\n",
    "indx = torch.cat((indx, index_next), dim=1)\n",
    "```\n",
    "\n",
    "- Appends the newly generated token (`index_next`) to the input sequence (`indx`) for the next iteration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Check for Stop Tokens**\n",
    "```python\n",
    "if stop_tokens is not None and index_next.item() in stop_tokens:\n",
    "    break\n",
    "```\n",
    "\n",
    "- If a stop token is generated, the function stops further token generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Return Generated Sequence**\n",
    "```python\n",
    "return indx\n",
    "```\n",
    "\n",
    "- Returns the complete sequence of tokens, including the original input and all newly generated tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features**\n",
    "1. **Context Trimming**: Handles sequences longer than the model's context length.\n",
    "2. **Temperature Scaling**: Balances randomness vs. determinism in token selection.\n",
    "3. **Top-k Sampling**: Ensures diversity by limiting token selection to top-k candidates.\n",
    "4. **Stop Tokens**: Enables early termination based on specific conditions.\n",
    "\n",
    "This function can be used to generate text from a GPT model with fine control over randomness, diversity, and termination criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(20, 48)\n",
       "  (pos_emb): Embedding(32, 48)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (out_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (out_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (W_value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (out_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=48, out_features=20, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(model_config)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"trained_model_1000_v1.pth\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove 'module.' prefix from the keys\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    # k[7:] is \"module.\" characters\n",
    "    name = k[7:] if k.startswith(\"module.\") else k  # remove 'module.' prefix in the keys\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "model.to(\"cpu\")  # Move the model to the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Model Initialization**\n",
    "```python\n",
    "model = GPTModel(model_config)\n",
    "```\n",
    "\n",
    "- **Creates a new GPT model instance** using `GPTModel` with the specified `model_config`.\n",
    "- This new instance will later be updated with the trained parameters from the saved model file.\n",
    "\n",
    "---\n",
    "\n",
    "### **Load the State Dictionary**\n",
    "```python\n",
    "state_dict = torch.load(\"trained_model_1000_v1.pth\", map_location=torch.device('cpu'))\n",
    "```\n",
    "\n",
    "- **Loads the saved model weights** from the file `trained_model_1000_v1.pth` into `state_dict`.\n",
    "- The `map_location=torch.device('cpu')` ensures that the weights are loaded onto the CPU, regardless of where they were originally saved (e.g., GPU).\n",
    "\n",
    "Using both `map_location=\"cpu\"` and `model.to(\"cpu\")` ensures that:\n",
    "\n",
    "1. **`map_location=\"cpu\"`**: Loads the saved model weights onto the CPU, regardless of where they were saved (GPU or CPU).\n",
    "2. **`model.to(\"cpu\")`**: Configures the model itself to run computations on the CPU.\n",
    "\n",
    "Together, they prevent device mismatches and ensure the model and weights align for consistent CPU-based inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adjust the Keys in the State Dictionary**\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith(\"module.\") else k  # remove 'module.' prefix in the keys\n",
    "    new_state_dict[name] = v\n",
    "```\n",
    "\n",
    "- **Problem**: The saved `state_dict` contains keys prefixed with `\"module.\"`, which occurs when models are trained using `torch.nn.DataParallel`. This prefix must be removed for compatibility with a non-`DataParallel` model.\n",
    "- **Solution**:\n",
    "  - Loops through all key-value pairs in the `state_dict`.\n",
    "  - Checks if a key starts with `\"module.\"`.\n",
    "  - If true, removes the prefix by slicing (`k[7:]`).\n",
    "  - Creates a new `state_dict` (`new_state_dict`) without the `\"module.\"` prefix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Load the Processed State Dictionary**\n",
    "```python\n",
    "model.load_state_dict(new_state_dict)\n",
    "```\n",
    "\n",
    "- Updates the initialized model (`model`) with the weights from the processed `new_state_dict`.\n",
    "- Ensures that the model now has the trained parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **Set Evaluation Mode**\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "- **Switches the model to evaluation mode**:\n",
    "  - Disables dropout and batch normalization layers, ensuring deterministic behavior during inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Move Model to CPU**\n",
    "```python\n",
    "model.to(\"cpu\")\n",
    "```\n",
    "\n",
    "- Moves the model's parameters and buffers to the CPU.\n",
    "- This step ensures compatibility with systems that don't have a GPU or when inference will occur on the CPU.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Steps**\n",
    "1. **Initialize the Model**: A new model instance is created using the desired configuration.\n",
    "2. **Load Weights**: The trained weights are loaded from a saved file.\n",
    "3. **Key Adjustment**: Any prefixes from multi-GPU training (`module.`) are removed to ensure compatibility.\n",
    "4. **Load State Dictionary**: The processed weights are loaded into the model.\n",
    "5. **Prepare for Evaluation**: The model is switched to evaluation mode and moved to the CPU.\n",
    "\n",
    "---\n",
    "\n",
    "### **Result**\n",
    "The `model` is now fully loaded with trained weights, in evaluation mode, and ready for inference on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user for two inputs\n",
    "a = 0\n",
    "b = 0\n",
    "# Create the input string\n",
    "input_str = f\"{a}+{b}=\"\n",
    "input_ids = torch.tensor(tokenizer.encode(input_str)[:-1], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "generated_tokens = generate(\n",
    "    model=model,\n",
    "    indx=input_ids,  # Starting token\n",
    "    max_new_tokens=32,  # Number of tokens to generate\n",
    "    temperature=1.0,\n",
    "    top_k=10,\n",
    "    stop_tokens=[tokenizer._convert_token_to_id(x) for x in [\"[SEP]\", \"[CLS]\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=0+0+0=0"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(generated_tokens[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "for token in tokens:\n",
    "    sys.stdout.write(token)  # Print the token horizontally\n",
    "    sys.stdout.flush()  # Ensure it's displayed immediately\n",
    "    time.sleep(0.2)  # Add a delay in seconds to slow down the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nepgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
